# -*- coding: utf-8 -*-
"""amazon_v2_cleanup.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NrXeB6fOqL2Rj6K7uhil0_mlpqwDS4Fj

# Data Pipeline mit dem Code in 1 Zelle
"""

# ETL-Programm zur Bereinigung der "messy" Amazon-Daten

# Installation

# !pip install pyspark

# Importieren

import pyspark, logging

# Import notwendiger Module

from pyspark.sql import SparkSession

from pyspark.sql.functions import (
    col, when, regexp_replace, udf, to_date, lit, upper, trim, length
)
from pyspark.sql.types import StringType, FloatType, IntegerType, BooleanType

from re import MULTILINE

# Spark-Session starten

# --> Ist dieser Schritt notwendig? Die/Eine Spark-Session wird ja auch in der Klasse gestartet!

spark = SparkSession.builder \
    .appName("ETL Pipeline for Data Cleaning") \
    .getOrCreate()

# Logger einrichten

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# ETL-Klasse mit Modularität

class ETLPipeline:
    def __init__(self, input_path, output_path):
        """Initialisiert die Pipeline mit den Pfaden und startet Spark."""
        self.spark = SparkSession.builder \
            .appName("ETL Pipeline for Data Cleaning") \
            .getOrCreate()
        self.input_path = input_path
        self.output_path = output_path
        self.df = None
        logging.info("ETL-Pipeline initialisiert.")

    def load_data(self):
        """Lädt die Daten aus der Eingabedatei."""
        # Die Eingabedatei enthält Zeilenumbrüche, daher muss hier die Option multiLine gesetzt werden
        self.df = spark.read.csv(self.input_path, header=True, inferSchema=True, multiLine=True)

        logging.info("Daten erfolgreich geladen.")

    def show_dataframe(self):
        """
        Gibt den Inhalt des DataFrames mit der show-Funktion aus.
        """
        self.df.show()

    def clean_spaces(self):
        """
        Entfernt führende und nachfolgende Leerzeichen aus allen Spalten.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame ohne führende/nachfolgende Leerzeichen.
        """

        return self.df.select([trim(col(c)).alias(c) for c in self.df.columns])

    def clean_field(self, fieldname, regex_string, field_mapping):

        def return_valid_value(self, s):

            # So wie ich das sehe wird diese Funktion nicht mehr benötigt,
            # da sie durch die field_udx ersetzt wird
            if s in set_of_valid_values:
                return(set_of_valid_values[s])
            else:
                return("#unknown")

        field_udf = udf(lambda s: field_mapping.get(s, "#unbekannt"), StringType())

        df_copy = self.df.withColumn(fieldname, upper(col(fieldname)))
        df_copy = df_copy.withColumn(fieldname, regexp_replace(col(fieldname), regex_string, ""))
        df_copy = df_copy.withColumn(fieldname, field_udf(col(fieldname)))
        return df_copy

    def clean_index(self):
        """
        Entfernt alle nicht-numerischen Zeichen aus der Spalte 'index'.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit bereinigter Spalte 'index'.
        """
        # Regex-Transformation: Erlaubt nur Zahlen in der 'index'-Spalte.
        return self.df.withColumn("index", regexp_replace(col("index"), "[^0-9]", ""))

    def validate_order_id(self):
        """
        Validiert die 'Order ID'-Spalte und entfernt ungültige Werte.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit gültigen 'Order ID'-Werten.
        """
        # Regex-Pattern für gültige Order IDs (z. B. "123-4567890-1234567").
        valid_order_id = r"^\d{3}-\d{7}-\d{7}$"

        # Anwenden der Validierung; ungültige Werte werden durch None ersetzt.
        df_copy = self.df.withColumn(
            "Order ID",
            when(col("Order ID").rlike(valid_order_id), col("Order ID")).otherwise(None)
        )

        # Entfernen aller Zeilen mit ungültigen (None) Order IDs.
        return df_copy.na.drop(subset=["Order ID"])

    def normalize_date(self):
        """
        Normalisiert und validiert die Spalte 'Date'.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit gültigen Datumswerten.
        """
        # Definiere mögliche Muster und das gewünschte Ausgabeformat.
        date_patterns = [
            ("^\d{4}-\d{2}-\d{2}$", "yyyy-MM-dd"),  # Beispiel: 2023-10-01
            ("^\d{2}/\d{2}/\d{4}$", "MM/dd/yyyy"),  # Beispiel: 10/01/2023
            ("^\d{2}-\d{2}-\d{4}$", "dd-MM-yyyy")   # Beispiel: 01-10-2023
        ]

        # Iteriere über die Muster und wende sie auf die 'Date'-Spalte an.
        for pattern, date_format in date_patterns:
            df_copy = self.df.withColumn(
                "Date",
                when(col("Date").rlike(pattern), to_date(col("Date"), date_format)).otherwise(col("Date"))
            )

        # Konvertiere verbleibende Werte ins Datumsformat und entferne ungültige Datensätze.
        return df_copy.withColumn("Date", to_date(col("Date"))).na.drop(subset=["Date"])

    def clean_status(self):
        """
        Bereinigt und mappt die Spalte 'Status' auf gültige Werte.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit bereinigtem 'Status'.
        """

        # Mapping von gültigen Statuswerten:
        status_mapping = {
            "CANCELLED": "Cancelled",
            "SHIPPEDDELIVEREDTOBUYER": "Shipped - Delivered to Buyer",
            "SHIPPED": "Shipped",
            "SHIPPEDRETURNEDTOSELLER": "Shipped - Returned to Seller",
            "SHIPPEDREJECTEDBYBUYER": "Shipped - Rejected by Buyer",
            "SHIPPEDLOSTINTRANSIT": "Shipped - Lost in Transit",
            "SHIPPEDOUTFORDELIVERY": "Shipped - Out for Delivery",
            "SHIPPEDRETURNINGTOSELLER": "Shipped - Returning to Seller",
            "SHIPPEDPICKEDUP": "Shipped - Picked Up",
            "PENDING": "Pending",
            "PENDINGWAITINGFORPICKUP": "Pending - Waiting for Pick Up",
            "SHIPPEDDAMAGED": "Shipped - Damaged",
            "SHIPPING": "Shipping"
        }

        # UDF (User Defined Function), um das Mapping anzuwenden.
        status_udf = udf(lambda s: status_mapping.get(s, "#unknown"), StringType())

        # Spalte 'Status' bereinigen und transformieren.
        # Original-Befehle:
        # self.df = self.df.withColumn("Status", upper(trim(col("Status"))))
        # self.df = self.df.withColumn("Status", status_udf(col("Status")))

        df_copy = self.df.withColumn("Status", upper(col("Status")))
        df_copy = df_copy.withColumn("Status", regexp_replace(col("Status"), "[^A-Z]", ""))
        df_copy = df_copy.withColumn("Status", status_udf(col("Status")))
        return df_copy

    def clean_fulfilment(self):
        """
        Bereinigt und validiert die Spalte 'Fulfilment'.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit bereinigtem 'Fulfilment'.
        """
        # Mapping von gültigen Fulfilment-Werten.
        fulfilment_mapping = {"AMAZON": "Amazon", "MERCHANT": "Merchant"}

        # UDF für die Anwendung des Mappings.
        fulfilment_udf = udf(lambda s: fulfilment_mapping.get(s, "#unknown"), StringType())

        # Transformation der 'Fulfilment'-Spalte.
        df_copy = self.df.withColumn("Fulfilment", upper(col("Fulfilment")))
        return df_copy.withColumn("Fulfilment", fulfilment_udf(col("Fulfilment")))

    def clean_sales_channel(self):
        """
        Bereinigt und mappt die Spalte 'Sales Channel'.

        - Mögliche Werte: AMAZONIN → Amazon.in, NONAMAZON → Non-Amazon
        - Unbekannte Werte werden auf #unknown gesetzt.
        """
        # Definiere ein Mapping für gültige Sales Channel-Werte.
        channel_mapping = {
            "AMAZONIN": "Amazon.in",
            "NONAMAZON": "Non-Amazon"
        }

        # UDF für das Mapping erstellen.
        channel_udf = udf(lambda s: channel_mapping.get(s, "#unknown"), StringType())

        # Transformation anwenden: Bereinigen und Mappen der Werte.
        df_copy = self.df.withColumn("Sales Channel", upper(col("Sales Channel")))
        df_copy = df_copy.withColumn("Sales Channel", regexp_replace(col("Sales Channel"), "[^A-Z]", ""))
        return df_copy.withColumn("Sales Channel", channel_udf(col("Sales Channel")))

    def clean_ship_service_level(self):
        """
        Bereinigt und mappt die Spalte 'ship-service-level'.

        - Mögliche Werte: STANDARD → Standard, EXPEDITED → Expedited
        - Unbekannte Werte werden auf #unknown gesetzt.
        """
        # Definiere ein Mapping für gültige ship-service-level-Werte.
        service_mapping = {"STANDARD": "Standard", "EXPEDITED": "Expedited"}

        # UDF für das Mapping erstellen.
        service_udf = udf(lambda s: service_mapping.get(s, "#unknown"), StringType())

        # Transformation anwenden: Bereinigen und Mappen der Werte.
        df_copy = self.df.withColumn("ship-service-level", upper(col("ship-service-level")))
        return df_copy.withColumn("ship-service-level", service_udf(col("ship-service-level")))

    def clean_category(self):
        """
        Bereinigt und mappt die Spalte 'Category' auf gültige Werte.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit bereinigter 'Category'.
        """

        # Mapping von gültigen Statuswerten:
        category_mapping = {
            "TSHIRT": "T-shirt",
            "SHIRT": "Shirt",
            "BLAZZER": "Blazzer",
            "TROUSERS": "Trousers",
            "PERFUME": "Perfume",
            "SOCKS": "Socks",
            "SHOES": "Shoes",
            "WALLET": "Wallet",
            "WATCH": "Watch"
        }

        # UDF (User Defined Function), um das Mapping anzuwenden.
        category_udf = udf(lambda s: category_mapping.get(s, "#unknown"), StringType())

        df_copy = self.df.withColumn("Category", upper(col("Category")))
        df_copy = df_copy.withColumn("Category", regexp_replace(col("Category"), "[^A-Z]", ""))
        df_copy = df_copy.withColumn("Category", category_udf(col("Category")))
        return df_copy

    def clean_size(self):
        """
        Bereinigt und mappt die Spalte 'Size' auf gültige Werte.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit bereinigter 'Size'.
        """

        # Mapping von gültigen Statuswerten:
        size_mapping = {
            "S": "S",
            "3XL": "3XL",
            "XL": "XL",
            "L": "L",
            "XXL": "XXL",
            "XS": "XS",
            "6XL": "6XL",
            "M": "M",
            "4XL": "4XL",
            "FREE": "Free",
            "5XL": "5XL"
        }

        # UDF (User Defined Function), um das Mapping anzuwenden.
        size_udf = udf(lambda s: size_mapping.get(s, "#unknown"), StringType())

        df_copy = self.df.withColumn("Size", upper(col("Size")))
        df_copy = df_copy.withColumn("Size", regexp_replace(col("Size"), "[^3456EFLMRSX]", ""))
        df_copy = df_copy.withColumn("Size", size_udf(col("Size")))
        return df_copy

    def clean_Courier_Status_old(self):
        """
        Bereinigt und mappt die Spalte 'Courier Status' auf gültige Werte.

        Parameters:df_cleaned = df.withColumn(
    "ship-postal-code",
    when(length(col("ship-postal-code")) == 6, col("ship-postal-code")).otherwise("")
)
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit bereinigtem 'Courier Status'.
        """

        # Mapping von gültigen Statuswerten:
        Courier_Status_mapping = {
            "CANCELLED": "Cancelled",
            "ONTHEWAY": "On the Way",
            "SHIPPED": "Shipped",
            "UNSHIPPED": "Unshipped"
        }

        # UDF (User Defined Function), um das Mapping anzuwenden.
        Courier_Status_udf = udf(lambda s: Courier_Status_mapping.get(s, "#unknown"), StringType())

        df_copy = self.df.withColumn("Courier Status", upper(col("Courier Status")))
        df_copy = df_copy.withColumn("Courier Status", regexp_replace(col("Courier Status"), "[^A-Z]", ""))
        df_copy = df_copy.withColumn("Courier Status", Courier_Status_udf(col("Courier Status")))
        return df_copy

    def clean_courier_status(self):

        # Mapping von gültigen Statuswerten:
        Courier_Status_mapping = {
            "CANCELLED": "Cancelled",
            "ONTHEWAY": "On the Way",
            "SHIPPED": "Shipped",
            "UNSHIPPED": "Unshipped"
        }
        return self.clean_field("Courier Status", "^A-Z", Courier_Status_mapping)

    def clean_qty(self):
        """
        versucht, aus dem Feld Qty ei
        ne natürliche Zahl > 0 zu extrahieren
        indem zuerst alle Zeichen entfernt werden, die keine Ziffern sind
        und dann die Zahl als Integer interpretiert wird.
        Wenn die Zahl <= 0 ist, dann wird der Datensatz entfernt

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit gültigem Qty-Wert
        """

        # Schritt 1: Entferne alle Zeichen, die keine Ziffern sind
        # Verwende eine reguläre Ausdrucksfunktion, um nur Ziffern zu behalten.
        df_copy = self.df.withColumn("Qty", regexp_replace(col("Qty"), "[^0-9]", ""))

        # Schritt 2: Konvertiere den bereinigten "qty_cleaned"-Wert in einen Integer
        df_copy = df_copy.withColumn("Qty", col("Qty").cast("int"))

        # Schritt 3: Lösche die Datensätze, bei denen Qty <= 0 ist
        df_copy = df_copy.filter(col("Qty") > 0)

        return df_copy

    def clean_currency(self):

        # Mapping von gültigen Währungen:
        currency_mapping = {
            "DOLLAR": "USD",
            "$": "USD",
            "EUR": "EUR",
            "EURO": "EUR",
            "€": "EUR",
            "RUPEES": "INR",
            "Rs": "INR",
            "₹": "INR"
        }
        return self.clean_field("currency", "A-Z€₹$", currency_mapping)

    def clean_amount(self):
        """
        Bereinigt die Spalte 'amount'.
        Entfernt alle Zeichen, die nicht in "0123456789." enthalten sind,
        entfernt alle Inhalte, die nicht vom Typ Float sind und
        wandelt das Feld in Float um

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit bereinigtem 'amount'.
        """

        # 1) Entferne alle Zeichen, die nicht in "0123456789." sind
        df_copy = self.df.withColumn("amount_cleaned", regexp_replace(col("amount"), "[^0-9.]", ""))

        # 2) Konvertiere die Spalte in den Float-Typ
        df_copy = df_copy.withColumn("amount_float", col("amount_cleaned").cast(FloatType()))

        # 3) Entferne Zeilen mit ungültigen Zahlen
        # df_copy = df_copy.filter(col("amount_float").isNotNull())

        return df_copy

    def clean_ship_city(self):
        """
        Bereinigt die Spalte 'ship-city'.
        Alle Zeichen außer Buchstaben und dem Leerzeichen werden entfernt.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit bereinigter 'ship-city'.
        """

        return self.df.withColumn("ship-city", regexp_replace(upper(col("ship-city")), "[^A-Z ]", ""))

    def clean_ship_state(self):

        # Mapping von gültigen State-Werten:
        ship_state_mapping = {
            "ANDAMANNICOBAR": "ANDAMAN & NICOBAR",
            "ANDHRAPRADESH": "ANDHRA PRADESH",
            "APO": "APO",
            "AR": "AR",
            "ARUNACHALPRADESH": "ARUNACHAL PRADESH",
            "ASSAM": "ASSAM",
            "BIHAR": "BIHAR",
            "CHANDIGARH": "CHANDIGARH",
            "CHHATTISGARH": "CHHATTISGARH",
            "DADRAANDNAGAR": "DADRA AND NAGAR",
            "DELHI": "DELHI",
            "GOA": "GOA",
            "GOLAPGONJ": "GOLAPGONJ",
            "GUJARAT": "GUJARAT",
            "GURUVAYOOR": "GURUVAYOOR",
            "HARYANA": "HARYANA",
            "HIMACHALPRADESH": "HIMACHAL PRADESH",
            "JAMMUKASHMIR": "JAMMU & KASHMIR",
            "JHARKHAND": "JHARKHAND",
            "KARNATAKA": "KARNATAKA",
            "KERALA": "KERALA",
            "LADAKH": "LADAKH",
            "LAKSHADWEEP": "LAKSHADWEEP",
            "MADHYAPRADESH": "MADHYA PRADESH",
            "MAHARASHTRA": "MAHARASHTRA",
            "MAHESHPURPRIMARYSCHOOL": "MAHESHPUR PRIMARY SCHOOL",
            "MANIPUR": "MANIPUR",
            "MEGHALAYA": "MEGHALAYA",
            "MIZORAM": "MIZORAM",
            "NAGALAND": "NAGALAND",
            "NEWDELHI": "NEW DELHI",
            "NL": "NL",
            "ODISHA": "ODISHA",
            "ORISSA": "ORISSA",
            "PB": "PB",
            "PONDICHERRY": "PONDICHERRY",
            "PRATAPGARH": "PRATAPGARH",
            "PUDUCHERRY": "PUDUCHERRY",
            "PUNJAB": "PUNJAB",
            "PUNJABMOHALIZIRAKPUR": "PUNJAB/MOHALI/ZIRAKPUR",
            "RAJASTHAN": "RAJASTHAN",
            "RAJSHTHAN": "RAJSHTHAN",
            "RAJSTHAN": "RAJSTHAN",
            "RJ": "RJ",
            "SIKKIM": "SIKKIM",
            "TAMILNADU": "TAMIL NADU",
            "TELANGANA": "TELANGANA",
            "TRIPURA": "TRIPURA",
            "UTTARPRADESH": "UTTAR PRADESH",
            "UTTARAKHAND": "UTTARAKHAND",
            "WESTBENGAL": "WEST BENGAL"
        }
        return self.clean_field("ship-state", "^A-Z", ship_state_mapping)

    def clean_ship_postal_code(self):
        """
        Bereinigt die Spalte 'ship-postal-code'.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit bereinigtem 'ship-postal-code'.
        """

        return self.df.withColumn("ship-postal-code",
            when(length(col("ship-postal-code")) == 6, col("ship-postal-code")).otherwise("")
)
        return df_copy

    def clean_ship_country(self):

        # Mapping von gültigen ship-country Werten:
        ship_country_mapping = {
            "IN": "IN"
        }
        return self.clean_field("ship-country", "^A-Z", ship_country_mapping)

    def clean_b2b(self):

        # Mapping von gültigen B2B-Werten:
        B2B_mapping = {
            "TRUE": "TRUE",
            "FALSE": "FALSE"
        }
        return self.clean_field("B2B", "^A-Z", B2B_mapping)

    def clean_fulfilled_by(self):

        # Mapping von gültigen B2B-Werten:
        fulfilled_by_mapping = {
            "EASYSHIP": "Easy Ship"
        }
        return self.clean_field("fulfilled-by", "^A-Z", fulfilled_by_mapping)

    def clean_new(self):
        """
        Bereinigt die Spalte 'New'.
        Dieses Feld/diese Spalte ist im Original immer leer,
        daher wird sie hier vollständig gelöscht.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit leerem Feld 'New'.
        """

        # Das Feld 'New' wird vollständig mit dem Literal "" gefüllt
        return self.df.withColumn("New", lit(""))

    def clean_pendings(self):
        """
        Bereinigt die Spalte 'PendingS'.
        Dieses Feld/diese Spalte ist im Original immer leer,
        daher wird sie hier vollständig gelöscht.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit leerem Feld 'PendingS'.
        """

        # Das Feld 'PendingS' wird vollständig mit dem Literal "" gefüllt
        return self.df.withColumn("PendingS", lit("") )

    def clean_delivery_address(self):
        """
        Bereinigt die Spalte 'Delivery_Address'.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit bereinigtem 'Delivery_Address'.
        """
        return self.df.withColumn("Delivery_Address", regexp_replace(upper(col("Delivery_Address")), "[^A-Z0-9 -]", ""))

    def clean_customer_name(self):
        """
        Bereinigt die Spalte 'Customer_Name'.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit bereinigtem 'Customer_Name'.
        """
        return self.df.withColumn("Customer_Name", regexp_replace(upper(col("Customer_Name")), "[^A-Z0-9 -]", ""))

    def clean_phone_number(self):
        """
        Bereinigt die Spalte 'Phone_Number'.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit bereinigtem 'Phone_Number'.
        """

        df_copy = self.df.withColumn("Phone_Number", regexp_replace(col("Phone_Number"), "[^0-9]", ""))

        df_copy = df_copy.withColumn("Phone_Number", when(length(col("Phone_Number")) >= 7, col("Phone_Number")).otherwise(""))

        return df_copy

    def save_data(self):
        """Speichert die transformierten Daten."""

        # output_path = "amazon_test_cleaned_out.csv"

        self.df.write.mode('overwrite').csv(self.output_path, header=True)

        logging.info(f"Bereinigte Daten wurden in {self.output_path} gespeichert.")

    def run(self):
        """Führt die gesamte Pipeline aus."""
        logging.info("Startet die ETL-Pipeline.")

        self.load_data()

        self.df = self.clean_spaces()
        self.df = self.clean_index()
        self.df = self.validate_order_id()
        self.df = self.normalize_date()
        self.df = self.clean_status()
        self.df = self.clean_fulfilment()
        self.df = self.clean_sales_channel()
        self.df = self.clean_ship_service_level()
        self.df = self.clean_category()
        self.df = self.clean_size()
        self.df = self.clean_courier_status()
        self.df = self.clean_qty()
        self.df = self.clean_currency()
        self.df = self.clean_amount()
        self.df = self.clean_ship_city()
        self.df = self.clean_ship_state()
        self.df = self.clean_ship_postal_code()
        self.df = self.clean_ship_country()
        self.df = self.clean_b2b()
        self.df = self.clean_fulfilled_by()
        self.df = self.clean_new()
        self.df = self.clean_pendings()
        self.df = self.clean_delivery_address()
        self.df = self.clean_customer_name()
        self.df = self.clean_phone_number()

        self.save_data()
        logging.info("ETL-Pipeline erfolgreich abgeschlossen.")

"""# Einstiegspunkt der Pipeline"""

import logging
# Original:
# from etl_pipeline import ETLPipeline  # Importiere die ETL-Klasse

# import ETLPipeline  # Importiere die ETL-Klasse

# Logging einrichten
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),  # Logs auf der Konsole
        logging.FileHandler("etl_pipeline.log")  # Logs in einer Datei
    ]
)

def main():
    """
    Initialisiert und startet die ETL-Pipeline.
    """

    logging.info("Starte die Initialisierung der ETL-Pipeline.")

    # Pfade für Eingabe- und Ausgabedateien definieren
    input_path = "/home/maxpower/airflow/dags/amazon_test_messy_in.csv"
    output_path = "/home/maxpower/airflow/dags/amazon_test_cleaned_out.csv"

    # Instanziiere die ETL-Klasse
    pipeline = ETLPipeline(input_path, output_path)

    # Pipeline ausführen
    try:
        logging.info("try-started")
        pipeline.run()
        logging.info("ETL-Pipeline erfolgreich abgeschlossen.")
    except Exception as e:
        logging.error(f"Fehler während der Pipeline-Ausführung: {e}")
        raise

if __name__ == "__main__":
    main()

