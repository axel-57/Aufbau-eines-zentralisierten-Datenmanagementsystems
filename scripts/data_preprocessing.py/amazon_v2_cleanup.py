# -*- coding: utf-8 -*-
"""amazon_v2_cleanup.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NrXeB6fOqL2Rj6K7uhil0_mlpqwDS4Fj

# Data Pipeline mit dem Code in 1 Zelle
"""

# ETL-Programm zur Bereinigung der "messy" Amazon-Daten

# Installation

!pip install pyspark

# Importieren

import pyspark, logging

# Import notwendiger Module

from pyspark.sql import SparkSession

from pyspark.sql.functions import (
    col, when, regexp_replace, udf, to_date, lit, upper, trim
)
from pyspark.sql.types import StringType, FloatType, IntegerType, BooleanType

from re import MULTILINE

# Spark-Session starten

# --> Ist dieser Schritt notwendig? Die/Eine Spark-Session wird ja auch in der Klasse gestartet!

spark = SparkSession.builder \
    .appName("ETL Pipeline for Data Cleaning") \
    .getOrCreate()

# Logger einrichten

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# ETL-Klasse mit Modularität

class ETLPipeline:
    def __init__(self, input_path, output_path):
        """Initialisiert die Pipeline mit den Pfaden und startet Spark."""
        self.spark = SparkSession.builder \
            .appName("ETL Pipeline for Data Cleaning") \
            .getOrCreate()
        self.input_path = input_path
        self.output_path = output_path
        self.df = None
        logging.info("ETL-Pipeline initialisiert.")

    def load_data(self):
        """Lädt die Daten aus der Eingabedatei."""
        # Die Eingabedatei enthält Zeilenumbrüche, daher muss hier die Option multiLine gesetzt werden
        self.df = spark.read.csv(self.input_path, header=True, inferSchema=True, multiLine=True)

        logging.info("Daten erfolgreich geladen.")

    def show_dataframe(self):
        """
        Gibt den Inhalt des DataFrames mit der show-Funktion aus.
        """
        self.df.show()

    def clean_spaces(self):
        """
        Entfernt führende und nachfolgende Leerzeichen aus allen Spalten.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame ohne führende/nachfolgende Leerzeichen.
        """

        return self.df.select([trim(col(c)).alias(c) for c in self.df.columns])

    def clean_field(self, fieldname, regex_string, field_mapping):

        def return_valid_value(self, s):

            # So wie ich das sehe wird diese Funktion nicht mehr benötigt,
            # da sie durch die field_udx ersetzt wird
            if s in set_of_valid_values:
                return(set_of_valid_values[s])
            else:
                return("#unknown")

        field_udf = udf(lambda s: field_mapping.get(s, "#unknown"), StringType())

        df_copy = self.df.withColumn(fieldname, upper(col(fieldname)))
        df_copy = df_copy.withColumn(fieldname, regexp_replace(col(fieldname), regex_string, ""))
        df_copy = df_copy.withColumn(fieldname, field_udf(col(fieldname)))
        return df_copy

    def clean_index(self):
        """
        Entfernt alle nicht-numerischen Zeichen aus der Spalte 'index'.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit bereinigter Spalte 'index'.
        """
        # Regex-Transformation: Erlaubt nur Zahlen in der 'index'-Spalte.
        return self.df.withColumn("index", regexp_replace(col("index"), "[^0-9]", ""))

    def validate_order_id(self):
        """
        Validiert die 'Order ID'-Spalte und entfernt ungültige Werte.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit gültigen 'Order ID'-Werten.
        """
        # Regex-Pattern für gültige Order IDs (z. B. "123-4567890-1234567").
        valid_order_id = r"^\d{3}-\d{7}-\d{7}$"

        # Anwenden der Validierung; ungültige Werte werden durch None ersetzt.
        df_copy = self.df.withColumn(
            "Order ID",
            when(col("Order ID").rlike(valid_order_id), col("Order ID")).otherwise(None)
        )

        # Entfernen aller Zeilen mit ungültigen (None) Order IDs.
        return df_copy.na.drop(subset=["Order ID"])

    def normalize_date(self):
        """
        Normalisiert und validiert die Spalte 'Date'.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit gültigen Datumswerten.
        """
        # Definiere mögliche Muster und das gewünschte Ausgabeformat.
        date_patterns = [
            ("^\d{4}-\d{2}-\d{2}$", "yyyy-MM-dd"),  # Beispiel: 2023-10-01
            ("^\d{2}/\d{2}/\d{4}$", "MM/dd/yyyy"),  # Beispiel: 10/01/2023
            ("^\d{2}-\d{2}-\d{4}$", "dd-MM-yyyy")   # Beispiel: 01-10-2023
        ]

        # Iteriere über die Muster und wende sie auf die 'Date'-Spalte an.
        for pattern, date_format in date_patterns:
            df_copy = self.df.withColumn(
                "Date",
                when(col("Date").rlike(pattern), to_date(col("Date"), date_format)).otherwise(col("Date"))
            )

        # Konvertiere verbleibende Werte ins Datumsformat und entferne ungültige Datensätze.
        return df_copy.withColumn("Date", to_date(col("Date"))).na.drop(subset=["Date"])

    def clean_status(self):
        """
        Bereinigt und mappt die Spalte 'Status' auf gültige Werte.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit bereinigtem 'Status'.
        """

        # Mapping von gültigen Statuswerten:
        status_mapping = {
            "CANCELLED": "Cancelled",
            "SHIPPEDDELIVEREDTOBUYER": "Shipped - Delivered to Buyer",
            "SHIPPED": "Shipped",
            "SHIPPEDRETURNEDTOSELLER": "Shipped - Returned to Seller",
            "SHIPPEDREJECTEDBYBUYER": "Shipped - Rejected by Buyer",
            "SHIPPEDLOSTINTRANSIT": "Shipped - Lost in Transit",
            "SHIPPEDOUTFORDELIVERY": "Shipped - Out for Delivery",
            "SHIPPEDRETURNINGTOSELLER": "Shipped - Returning to Seller",
            "SHIPPEDPICKEDUP": "Shipped - Picked Up",
            "PENDING": "Pending",
            "PENDINGWAITINGFORPICKUP": "Pending - Waiting for Pick Up",
            "SHIPPEDDAMAGED": "Shipped - Damaged",
            "SHIPPING": "Shipping"
        }

        # UDF (User Defined Function), um das Mapping anzuwenden.
        status_udf = udf(lambda s: status_mapping.get(s, "#unknown"), StringType())

        # Spalte 'Status' bereinigen und transformieren.
        # Original-Befehle:
        # self.df = self.df.withColumn("Status", upper(trim(col("Status"))))
        # self.df = self.df.withColumn("Status", status_udf(col("Status")))

        df_copy = self.df.withColumn("Status", upper(col("Status")))
        df_copy = df_copy.withColumn("Status", regexp_replace(col("Status"), "[^A-Z]", ""))
        df_copy = df_copy.withColumn("Status", status_udf(col("Status")))
        return df_copy

    def clean_fulfilment(self):
        """
        Bereinigt und validiert die Spalte 'Fulfilment'.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit bereinigtem 'Fulfilment'.
        """
        # Mapping von gültigen Fulfilment-Werten.
        fulfilment_mapping = {"AMAZON": "Amazon", "MERCHANT": "Merchant"}

        # UDF für die Anwendung des Mappings.
        fulfilment_udf = udf(lambda s: fulfilment_mapping.get(s, "#unknown"), StringType())

        # Transformation der 'Fulfilment'-Spalte.
        df_copy = self.df.withColumn("Fulfilment", upper(col("Fulfilment")))
        return df_copy.withColumn("Fulfilment", fulfilment_udf(col("Fulfilment")))

    def clean_sales_channel(self):
        """
        Bereinigt und mappt die Spalte 'Sales Channel'.

        - Mögliche Werte: AMAZONIN → Amazon.in, NONAMAZON → Non-Amazon
        - Unbekannte Werte werden auf #unknown gesetzt.
        """
        # Definiere ein Mapping für gültige Sales Channel-Werte.
        channel_mapping = {
            "AMAZONIN": "Amazon.in",
            "NONAMAZON": "Non-Amazon"
        }

        # UDF für das Mapping erstellen.
        channel_udf = udf(lambda s: channel_mapping.get(s, "#unknown"), StringType())

        # Transformation anwenden: Bereinigen und Mappen der Werte.
        df_copy = self.df.withColumn("Sales Channel", upper(col("Sales Channel")))
        df_copy = df_copy.withColumn("Sales Channel", regexp_replace(col("Sales Channel"), "[^A-Z]", ""))
        return df_copy.withColumn("Sales Channel", channel_udf(col("Sales Channel")))

    def clean_ship_service_level(self):
        """
        Bereinigt und mappt die Spalte 'ship-service-level'.

        - Mögliche Werte: STANDARD → Standard, EXPEDITED → Expedited
        - Unbekannte Werte werden auf #unknown gesetzt.
        """
        # Definiere ein Mapping für gültige ship-service-level-Werte.
        service_mapping = {"STANDARD": "Standard", "EXPEDITED": "Expedited"}

        # UDF für das Mapping erstellen.
        service_udf = udf(lambda s: service_mapping.get(s, "#unknown"), StringType())

        # Transformation anwenden: Bereinigen und Mappen der Werte.
        df_copy = self.df.withColumn("ship-service-level", upper(col("ship-service-level")))
        return df_copy.withColumn("ship-service-level", service_udf(col("ship-service-level")))

    def clean_category(self):
        """
        Bereinigt und mappt die Spalte 'Category' auf gültige Werte.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit bereinigter 'Category'.
        """

        # Mapping von gültigen Statuswerten:
        category_mapping = {
            "TSHIRT": "T-shirt",
            "SHIRT": "Shirt",
            "BLAZZER": "Blazzer",
            "TROUSERS": "Trousers",
            "PERFUME": "Perfume",
            "SOCKS": "Socks",
            "SHOES": "Shoes",
            "WALLET": "Wallet",
            "WATCH": "Watch"
        }

        # UDF (User Defined Function), um das Mapping anzuwenden.
        category_udf = udf(lambda s: category_mapping.get(s, "#unknown"), StringType())

        df_copy = self.df.withColumn("Category", upper(col("Category")))
        df_copy = df_copy.withColumn("Category", regexp_replace(col("Category"), "[^A-Z]", ""))
        df_copy = df_copy.withColumn("Category", category_udf(col("Category")))
        return df_copy

    def clean_size(self):
        """
        Bereinigt und mappt die Spalte 'Size' auf gültige Werte.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit bereinigter 'Size'.
        """

        # Mapping von gültigen Statuswerten:
        size_mapping = {
            "S": "S",
            "3XL": "3XL",
            "XL": "XL",
            "L": "L",
            "XXL": "XXL",
            "XS": "XS",
            "6XL": "6XL",
            "M": "M",
            "4XL": "4XL",
            "FREE": "Free",
            "5XL": "5XL"
        }

        # UDF (User Defined Function), um das Mapping anzuwenden.
        size_udf = udf(lambda s: size_mapping.get(s, "#unknown"), StringType())

        df_copy = self.df.withColumn("Size", upper(col("Size")))
        df_copy = df_copy.withColumn("Size", regexp_replace(col("Size"), "[^3456EFLMRSX]", ""))
        df_copy = df_copy.withColumn("Size", size_udf(col("Size")))
        return df_copy

    def clean_Courier_Status(self):
        """
        Bereinigt und mappt die Spalte 'Courier Status' auf gültige Werte.

        Parameters:
        - df (DataFrame): Eingabe-DataFrame

        Returns:
        - DataFrame: Transformierter DataFrame mit bereinigtem 'Courier Status'.
        """

        # Mapping von gültigen Statuswerten:
        Courier_Status_mapping = {
            "CANCELLED": "Cancelled",
            "ONTHEWAY": "On the Way",
            "SHIPPED": "Shipped",
            "UNSHIPPED": "Unshipped"
        }

        # UDF (User Defined Function), um das Mapping anzuwenden.
        Courier_Status_udf = udf(lambda s: Courier_Status_mapping.get(s, "#unknown"), StringType())

        df_copy = self.df.withColumn("Courier Status", upper(col("Courier Status")))
        df_copy = df_copy.withColumn("Courier Status", regexp_replace(col("Courier Status"), "[^A-Z]", ""))
        df_copy = df_copy.withColumn("Courier Status", Courier_Status_udf(col("Courier Status")))
        return df_copy

    def clean_Courier_Status2(self):

        # Mapping von gültigen Statuswerten:
        Courier_Status_mapping = {
            "CANCELLED": "Cancelled",
            "ONTHEWAY": "On the Way",
            "SHIPPED": "Shipped",
            "UNSHIPPED": "Unshipped"
        }
        return self.clean_field("Courier Status", "^A-Z", Courier_Status_mapping)






    # TODO: Die restlichen Methoden hier einfügen




    def save_data(self):
        """Speichert die transformierten Daten."""

        # output_path = "amazon_test_cleaned_out.csv"

        self.df.write.mode('overwrite').csv(self.output_path, header=True)

        logging.info(f"Bereinigte Daten wurden in {self.output_path} gespeichert.")

    def run(self):
        """Führt die gesamte Pipeline aus."""
        logging.info("Startet die ETL-Pipeline.")

        self.load_data()

        self.df = self.clean_spaces()
        self.df = self.clean_index()
        self.df = self.validate_order_id()
        self.df = self.normalize_date()
        self.df = self.clean_status()
        self.df = self.clean_fulfilment()
        self.df = self.clean_sales_channel()
        self.df = self.clean_ship_service_level()
        self.df = self.clean_category()
        self.df = self.clean_size()

        # self.df = self.clean_Courier_Status()

        self.df = self.clean_Courier_Status2()



        # TODO: Den restlichen Code hier einfügen



        self.save_data()
        logging.info("ETL-Pipeline erfolgreich abgeschlossen.")

"""# Einstiegspunkt der Pipeline"""

import logging
# Original:
# from etl_pipeline import ETLPipeline  # Importiere die ETL-Klasse

# import ETLPipeline  # Importiere die ETL-Klasse

# Logging einrichten
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),  # Logs auf der Konsole
        logging.FileHandler("etl_pipeline.log")  # Logs in einer Datei
    ]
)

def main():
    """
    Initialisiert und startet die ETL-Pipeline.
    """

    logging.info("Starte die Initialisierung der ETL-Pipeline.")

    # Pfade für Eingabe- und Ausgabedateien definieren
    input_path = "amazon_test_messy_in.csv"
    output_path = "amazon_test_cleaned_out.csv"

    # Instanziiere die ETL-Klasse
    pipeline = ETLPipeline(input_path, output_path)

    # Pipeline ausführen
    try:
        logging.info("try-started")
        pipeline.run()
        logging.info("ETL-Pipeline erfolgreich abgeschlossen.")
    except Exception as e:
        logging.error(f"Fehler während der Pipeline-Ausführung: {e}")
        raise

if __name__ == "__main__":
    main()